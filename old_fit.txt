

TSEC Conceptual framework

	How do we assume our data relate to our model world?

		\textbf{No error:} We could attempt to model everything we see, in
		exact detail

		\textbf{Observation error:} we could assume that the world is
		perfectly deterministic, but our \emph{observations} are imperfect

		\textbf{Process error:} we could assume that we observe perfectly,
		but that the world is stochastic

		\textbf{Both kinds of error:} the world is stochastic, and our
		observations are imperfect

----------------------------------------------------------------------

TSEC Distance functions

	We can fit a model to data by:

		Solving an equation

		By eye (fiddling with parameters)

		\emph{Minimizing a distance function}

		Likelihood

----------------------------------------------------------------------

Distance functions

BC

SCALEFIG 0.6 distance.Rout-0.png

NC

	$$D = \sum_i{y_i - \hat y_i}$$

	SCALEFIG 0.4 images/deer.jpg

EC

----------------------------------------------------------------------
Distance functions

BC

SCALEFIG 0.6 distance.Rout-1.png

NC

	$$D = \sum_i{|y_i - \hat y_i|}$$

	SCALEFIG 0.9 images/mpaka.jpg

EC

----------------------------------------------------------------------

Distance functions

BC

SCALEFIG 0.6 distance.Rout-2.png

NC

	$$D = \sum_i{(y_i - \hat y_i)^2}$$

	SCALEFIG 0.9 images/elegant.jpg

EC

----------------------------------------------------------------------

TSEC Likelihoods

	Assume that the difference between the estimate $\hat y_i$ and the
	data point $y_i$ is normally distributed.  What is the log
	likelihood?

	$$L = \prod_i{\frac{1}{\sigma\sqrt{2\pi}}\exp\left(\frac{-(\hat
	y_i-y_i)^2}{2\sigma^2}\right)}$$

	$$\ell = \sum_i{-\log(\sigma\sqrt{2\pi}) - \sum_i{\frac{(\hat
	y_i-y_i)^2}{2\sigma^2}}}$$

	\emph{We minimize the likelihood by minimizing the sum of squares}

		and then solving for $\sigma$

----------------------------------------------------------------------

Least squares $\to$ likelihood

BC

	Attaching your least squares fit to a likelihood means:

		You can \emph{use it} for statistical inference (LRT)

		You can \emph{challenge} the assumptions

NC

	SCALEFIG 0.5 images/spiderman.jpg

EC


----------------------------------------------------------------------

Mexican flu example

BC

SCALEFIG 0.9  fitting/mexican_plots.Rout-0.pdf

NC

	How fast is it growing? $r$

	How hard will it be to control? $\Ro$ 

EC

----------------------------------------------------------------------

A different perspective

BC

SCALEFIG  0.9 fitting/mexican_plots.Rout-1.pdf

NC

	We could make the normal assumption on either scale

	How much does it matter?

EC

----------------------------------------------------------------------

Normal assumption

BC

SCALEFIG 0.9 fitting/mexican_plots.Rout-2.pdf

NC

	Least squares on the linear scale

	10:50 :: 980:1020

	Gives relatively too much weight to large observations

EC

----------------------------------------------------------------------


BC

SCALEFIG 0.9 fitting/mexican_plots.Rout-2.pdf

NC

SCALEFIG 0.9 fitting/mexican_plots.Rout-3.pdf

EC

----------------------------------------------------------------------

Lognormal assumption

BC

SCALEFIG 0.9 fitting/mexican_plots.Rout-5.pdf

NC

	Least squares on the log scale

	3:5 :: 300:500

	Gives relatively too much weight to small observations

EC

----------------------------------------------------------------------

BC

SCALEFIG 0.9 fitting/mexican_plots.Rout-4.pdf

NC

SCALEFIG 0.9 fitting/mexican_plots.Rout-5.pdf

EC

----------------------------------------------------------------------

A more realistic error distribution

	If my model predicts 30% prevalence in a city of 10,000 people, what
	distribution should I use to calculate the likelihood?

		The binomial.

		This never works

		Why not?

----------------------------------------------------------------------

The world is a mess

BC

SCALEFIG 0.6 images/crowd.jpg

NC

	Binomial assumes the people you examine are independent (conditional
	on being in your population)

	\emph{Negative binomial} is the simplest way to relax this assumption

EC

----------------------------------------------------------------------


Negative binomial fits

BC

SCALEFIG 0.9 fitting/mexican_plots.Rout-8.pdf

NC

SCALEFIG 0.9 fitting/mexican_plots.Rout-9.pdf

EC

----------------------------------------------------------------------

Comparison

	Realistic error distribution provides (apparently) better fits

	Confidence intervals

		Normal: $r$ = 0.96--0.97/wk

		Lognormal: $r$ = 0.64--1.29/wk

		Negative binomial: $r$ = 0.90--1.14/wk

	How would you test this?

----------------------------------------------------------------------

Identifiability

BC

SCALEFIG 0.9  fitting/mexican_plots.Rout-0.pdf

NC

	What if we tried to estimate \Ro\ from data like these?

EC

----------------------------------------------------------------------

SEC Stepping, shooting and beyond

----------------------------------------------------------------------

Types of error

	Where does this model fit into our conceptual framework?

	How do you know?

	What would the other options look like?

----------------------------------------------------------------------

Types of model

BC

	Shooting models

		Run your model from beginning to end

		\emph{Then compare your points to data points}

		Models \emph{observation error} only

NC

	Stepping models

		Evaluate your likelihood at each step

		Start again with the \emph{observed} value at each point

		Models \emph{process error} only

EC

----------------------------------------------------------------------

Latent variables

	Why do people step and shoot?

	We can model both observation and process error if we treat the true
	number of cases as a hidden variable, and (somehow) consider all the
	values that it can take

		Conceptually: exactly what we want

		Practically: extremely difficult

	Other hidden variables:

		Number of susceptibles, asymptomatic cases

----------------------------------------------------------------------

Summary

	Likelihood fits allow great power, but the CIs are only as good as
	the assumptions

	Find your implicit assumptions (normality, constant variance) and
	challenge them

	Taking latent variables into account is very difficult, but can be
	extremely useful

